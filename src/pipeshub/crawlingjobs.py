"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from .basesdk import BaseSDK
from pipeshub import errors, models, utils
from pipeshub._hooks import HookContext
from pipeshub.types import OptionalNullable, UNSET
from pipeshub.utils import get_security_from_env
from pipeshub.utils.unmarshal_json_response import unmarshal_json_response
from typing import Mapping, Optional, Union


class CrawlingJobs(BaseSDK):
    def schedule(
        self,
        *,
        connector: str,
        connector_id: str,
        schedule_config: Union[models.ScheduleConfig, models.ScheduleConfigTypedDict],
        priority: Optional[int] = 5,
        max_retries: Optional[int] = 3,
        timeout: Optional[int] = 300000,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.ScheduleCrawlingJobResponse:
        r"""Schedule a crawling job

        Schedule a new crawling job for a specific connector instance.<br><br>

        <b>Overview:</b><br>
        Creates a scheduled crawling job that will sync data from the specified connector into
        PipesHub's search index. The job is added to a BullMQ queue and will execute according
        to the specified schedule configuration.<br><br>

        <b>Schedule Types:</b><br>
        <ul>
        <li><b>hourly:</b> Run every X hours at specified minute (e.g., every 2 hours at :30)</li>
        <li><b>daily:</b> Run once per day at specified time (e.g., 2:00 AM daily)</li>
        <li><b>weekly:</b> Run on specific days of the week (e.g., Mon/Wed/Fri at 3:00 AM)</li>
        <li><b>monthly:</b> Run on specific day of month (e.g., 1st of each month at 4:00 AM)</li>
        <li><b>custom:</b> Use cron expression for complex schedules</li>
        <li><b>once:</b> Run once at a specific future datetime</li>
        </ul>

        <b>Access Control:</b><br>
        <ul>
        <li>Team-scoped connectors: Requires admin privileges</li>
        <li>Personal-scoped connectors: Only the creator can schedule jobs</li>
        </ul>

        <b>Job Behavior:</b><br>
        <ul>
        <li>If a job already exists for this connector, it will be replaced</li>
        <li>Disabled schedules (<code>isEnabled: false</code>) will throw an error</li>
        <li>Jobs use exponential backoff for retries (5s, 10s, 20s, etc.)</li>
        <li>Only last 10 completed/failed jobs are retained per connector</li>
        </ul>

        <b>Related Endpoints:</b><br>
        <ul>
        <li><code>GET /crawlingManager/{connector}/{connectorId}/schedule</code> - Get job status</li>
        <li><code>POST /crawlingManager/{connector}/{connectorId}/pause</code> - Pause job</li>
        <li><code>DELETE /crawlingManager/{connector}/{connectorId}/remove</code> - Remove job</li>
        </ul>


        :param connector: Connector type identifier (e.g., \"drive\", \"onedrive\", \"slack\", \"jira\")
        :param connector_id: Unique identifier of the connector instance (MongoDB ObjectId)
        :param schedule_config: Schedule configuration for crawling jobs. The structure varies based on <code>scheduleType</code>.<br><br>
            <b>Schedule Type Configurations:</b><br>
            <ul>
            <li><b>hourly:</b> <code>minute</code>, <code>interval</code> (optional)</li>
            <li><b>daily:</b> <code>hour</code>, <code>minute</code></li>
            <li><b>weekly:</b> <code>daysOfWeek</code>, <code>hour</code>, <code>minute</code></li>
            <li><b>monthly:</b> <code>dayOfMonth</code>, <code>hour</code>, <code>minute</code></li>
            <li><b>custom:</b> <code>cronExpression</code>, <code>description</code> (optional)</li>
            <li><b>once:</b> <code>scheduledTime</code></li>
            </ul>

        :param priority: Job priority (1=highest, 10=lowest). Higher priority jobs are processed first.
        :param max_retries: Maximum number of retry attempts on failure
        :param timeout: Job timeout in milliseconds (default 5 minutes)
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.ScheduleCrawlingJobRequest(
            connector=connector,
            connector_id=connector_id,
            body=models.ScheduleCrawlingJobRequestBody(
                schedule_config=utils.get_pydantic_model(
                    schedule_config, models.ScheduleConfig
                ),
                priority=priority,
                max_retries=max_retries,
                timeout=timeout,
            ),
        )

        req = self._build_request(
            method="POST",
            path="/crawlingManager/{connector}/{connectorId}/schedule",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request.body,
                False,
                False,
                "json",
                models.ScheduleCrawlingJobRequestBody,
            ),
            allow_empty_value=None,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="scheduleCrawlingJob",
                oauth2_scopes=None,
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["400", "401", "403", "404", "4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "201", "application/json"):
            return unmarshal_json_response(models.ScheduleCrawlingJobResponse, http_res)
        if utils.match_response(http_res, ["400", "401", "403", "404", "4XX"], "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise errors.PipeshubDefaultError(
                "API error occurred", http_res, http_res_text
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise errors.PipeshubDefaultError(
                "API error occurred", http_res, http_res_text
            )

        raise errors.PipeshubDefaultError("Unexpected response received", http_res)

    async def schedule_async(
        self,
        *,
        connector: str,
        connector_id: str,
        schedule_config: Union[models.ScheduleConfig, models.ScheduleConfigTypedDict],
        priority: Optional[int] = 5,
        max_retries: Optional[int] = 3,
        timeout: Optional[int] = 300000,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.ScheduleCrawlingJobResponse:
        r"""Schedule a crawling job

        Schedule a new crawling job for a specific connector instance.<br><br>

        <b>Overview:</b><br>
        Creates a scheduled crawling job that will sync data from the specified connector into
        PipesHub's search index. The job is added to a BullMQ queue and will execute according
        to the specified schedule configuration.<br><br>

        <b>Schedule Types:</b><br>
        <ul>
        <li><b>hourly:</b> Run every X hours at specified minute (e.g., every 2 hours at :30)</li>
        <li><b>daily:</b> Run once per day at specified time (e.g., 2:00 AM daily)</li>
        <li><b>weekly:</b> Run on specific days of the week (e.g., Mon/Wed/Fri at 3:00 AM)</li>
        <li><b>monthly:</b> Run on specific day of month (e.g., 1st of each month at 4:00 AM)</li>
        <li><b>custom:</b> Use cron expression for complex schedules</li>
        <li><b>once:</b> Run once at a specific future datetime</li>
        </ul>

        <b>Access Control:</b><br>
        <ul>
        <li>Team-scoped connectors: Requires admin privileges</li>
        <li>Personal-scoped connectors: Only the creator can schedule jobs</li>
        </ul>

        <b>Job Behavior:</b><br>
        <ul>
        <li>If a job already exists for this connector, it will be replaced</li>
        <li>Disabled schedules (<code>isEnabled: false</code>) will throw an error</li>
        <li>Jobs use exponential backoff for retries (5s, 10s, 20s, etc.)</li>
        <li>Only last 10 completed/failed jobs are retained per connector</li>
        </ul>

        <b>Related Endpoints:</b><br>
        <ul>
        <li><code>GET /crawlingManager/{connector}/{connectorId}/schedule</code> - Get job status</li>
        <li><code>POST /crawlingManager/{connector}/{connectorId}/pause</code> - Pause job</li>
        <li><code>DELETE /crawlingManager/{connector}/{connectorId}/remove</code> - Remove job</li>
        </ul>


        :param connector: Connector type identifier (e.g., \"drive\", \"onedrive\", \"slack\", \"jira\")
        :param connector_id: Unique identifier of the connector instance (MongoDB ObjectId)
        :param schedule_config: Schedule configuration for crawling jobs. The structure varies based on <code>scheduleType</code>.<br><br>
            <b>Schedule Type Configurations:</b><br>
            <ul>
            <li><b>hourly:</b> <code>minute</code>, <code>interval</code> (optional)</li>
            <li><b>daily:</b> <code>hour</code>, <code>minute</code></li>
            <li><b>weekly:</b> <code>daysOfWeek</code>, <code>hour</code>, <code>minute</code></li>
            <li><b>monthly:</b> <code>dayOfMonth</code>, <code>hour</code>, <code>minute</code></li>
            <li><b>custom:</b> <code>cronExpression</code>, <code>description</code> (optional)</li>
            <li><b>once:</b> <code>scheduledTime</code></li>
            </ul>

        :param priority: Job priority (1=highest, 10=lowest). Higher priority jobs are processed first.
        :param max_retries: Maximum number of retry attempts on failure
        :param timeout: Job timeout in milliseconds (default 5 minutes)
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.ScheduleCrawlingJobRequest(
            connector=connector,
            connector_id=connector_id,
            body=models.ScheduleCrawlingJobRequestBody(
                schedule_config=utils.get_pydantic_model(
                    schedule_config, models.ScheduleConfig
                ),
                priority=priority,
                max_retries=max_retries,
                timeout=timeout,
            ),
        )

        req = self._build_request_async(
            method="POST",
            path="/crawlingManager/{connector}/{connectorId}/schedule",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request.body,
                False,
                False,
                "json",
                models.ScheduleCrawlingJobRequestBody,
            ),
            allow_empty_value=None,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="scheduleCrawlingJob",
                oauth2_scopes=None,
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["400", "401", "403", "404", "4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "201", "application/json"):
            return unmarshal_json_response(models.ScheduleCrawlingJobResponse, http_res)
        if utils.match_response(http_res, ["400", "401", "403", "404", "4XX"], "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise errors.PipeshubDefaultError(
                "API error occurred", http_res, http_res_text
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise errors.PipeshubDefaultError(
                "API error occurred", http_res, http_res_text
            )

        raise errors.PipeshubDefaultError("Unexpected response received", http_res)

    def get_status(
        self,
        *,
        connector: str,
        connector_id: str,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.GetCrawlingJobStatusResponse:
        r"""Get crawling job status

        Retrieve the current status of a scheduled crawling job for a specific connector.<br><br>

        <b>Overview:</b><br>
        Returns detailed information about the most recent crawling job for the specified connector,
        including its current state, progress, timing information, and any error details.<br><br>

        <b>Job States:</b><br>
        <ul>
        <li><b>waiting:</b> Job is queued and waiting to be processed</li>
        <li><b>active:</b> Job is currently being processed by a worker</li>
        <li><b>completed:</b> Job finished successfully</li>
        <li><b>failed:</b> Job failed after exhausting retry attempts</li>
        <li><b>delayed:</b> Job is scheduled for future execution</li>
        <li><b>paused:</b> Job has been manually paused</li>
        </ul>

        <b>Access Control:</b><br>
        Same as scheduling - team connectors require admin, personal connectors require creator.


        :param connector: Connector type identifier
        :param connector_id: Unique identifier of the connector instance
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.GetCrawlingJobStatusRequest(
            connector=connector,
            connector_id=connector_id,
        )

        req = self._build_request(
            method="GET",
            path="/crawlingManager/{connector}/{connectorId}/schedule",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            allow_empty_value=None,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="getCrawlingJobStatus",
                oauth2_scopes=None,
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["401", "403", "404", "4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return unmarshal_json_response(
                models.GetCrawlingJobStatusResponse, http_res
            )
        if utils.match_response(http_res, ["401", "403", "404", "4XX"], "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise errors.PipeshubDefaultError(
                "API error occurred", http_res, http_res_text
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise errors.PipeshubDefaultError(
                "API error occurred", http_res, http_res_text
            )

        raise errors.PipeshubDefaultError("Unexpected response received", http_res)

    async def get_status_async(
        self,
        *,
        connector: str,
        connector_id: str,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.GetCrawlingJobStatusResponse:
        r"""Get crawling job status

        Retrieve the current status of a scheduled crawling job for a specific connector.<br><br>

        <b>Overview:</b><br>
        Returns detailed information about the most recent crawling job for the specified connector,
        including its current state, progress, timing information, and any error details.<br><br>

        <b>Job States:</b><br>
        <ul>
        <li><b>waiting:</b> Job is queued and waiting to be processed</li>
        <li><b>active:</b> Job is currently being processed by a worker</li>
        <li><b>completed:</b> Job finished successfully</li>
        <li><b>failed:</b> Job failed after exhausting retry attempts</li>
        <li><b>delayed:</b> Job is scheduled for future execution</li>
        <li><b>paused:</b> Job has been manually paused</li>
        </ul>

        <b>Access Control:</b><br>
        Same as scheduling - team connectors require admin, personal connectors require creator.


        :param connector: Connector type identifier
        :param connector_id: Unique identifier of the connector instance
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.GetCrawlingJobStatusRequest(
            connector=connector,
            connector_id=connector_id,
        )

        req = self._build_request_async(
            method="GET",
            path="/crawlingManager/{connector}/{connectorId}/schedule",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            allow_empty_value=None,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="getCrawlingJobStatus",
                oauth2_scopes=None,
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["401", "403", "404", "4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return unmarshal_json_response(
                models.GetCrawlingJobStatusResponse, http_res
            )
        if utils.match_response(http_res, ["401", "403", "404", "4XX"], "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise errors.PipeshubDefaultError(
                "API error occurred", http_res, http_res_text
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise errors.PipeshubDefaultError(
                "API error occurred", http_res, http_res_text
            )

        raise errors.PipeshubDefaultError("Unexpected response received", http_res)

    def remove(
        self,
        *,
        connector: str,
        connector_id: str,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.RemoveCrawlingJobResponse:
        r"""Remove a crawling job

        Permanently remove a scheduled crawling job for a specific connector.<br><br>

        <b>Overview:</b><br>
        Removes the crawling job and all associated data from the queue. This includes
        removing repeatable job configurations and cleaning up job history.<br><br>

        <b>What Gets Removed:</b><br>
        <ul>
        <li>Active or waiting job instances</li>
        <li>Repeatable job configuration (for recurring schedules)</li>
        <li>Paused job information</li>
        <li>Job mappings and metadata</li>
        </ul>

        <b>Note:</b> Completed and failed job records may be retained for audit purposes.

        <b>Related Endpoints:</b><br>
        <ul>
        <li><code>DELETE /crawlingManager/schedule/all</code> - Remove all jobs for organization</li>
        </ul>


        :param connector: Connector type identifier
        :param connector_id: Unique identifier of the connector instance
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.RemoveCrawlingJobRequest(
            connector=connector,
            connector_id=connector_id,
        )

        req = self._build_request(
            method="DELETE",
            path="/crawlingManager/{connector}/{connectorId}/remove",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            allow_empty_value=None,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="removeCrawlingJob",
                oauth2_scopes=None,
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["401", "403", "404", "4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return unmarshal_json_response(models.RemoveCrawlingJobResponse, http_res)
        if utils.match_response(http_res, ["401", "403", "404", "4XX"], "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise errors.PipeshubDefaultError(
                "API error occurred", http_res, http_res_text
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise errors.PipeshubDefaultError(
                "API error occurred", http_res, http_res_text
            )

        raise errors.PipeshubDefaultError("Unexpected response received", http_res)

    async def remove_async(
        self,
        *,
        connector: str,
        connector_id: str,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.RemoveCrawlingJobResponse:
        r"""Remove a crawling job

        Permanently remove a scheduled crawling job for a specific connector.<br><br>

        <b>Overview:</b><br>
        Removes the crawling job and all associated data from the queue. This includes
        removing repeatable job configurations and cleaning up job history.<br><br>

        <b>What Gets Removed:</b><br>
        <ul>
        <li>Active or waiting job instances</li>
        <li>Repeatable job configuration (for recurring schedules)</li>
        <li>Paused job information</li>
        <li>Job mappings and metadata</li>
        </ul>

        <b>Note:</b> Completed and failed job records may be retained for audit purposes.

        <b>Related Endpoints:</b><br>
        <ul>
        <li><code>DELETE /crawlingManager/schedule/all</code> - Remove all jobs for organization</li>
        </ul>


        :param connector: Connector type identifier
        :param connector_id: Unique identifier of the connector instance
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.RemoveCrawlingJobRequest(
            connector=connector,
            connector_id=connector_id,
        )

        req = self._build_request_async(
            method="DELETE",
            path="/crawlingManager/{connector}/{connectorId}/remove",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            allow_empty_value=None,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="removeCrawlingJob",
                oauth2_scopes=None,
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["401", "403", "404", "4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return unmarshal_json_response(models.RemoveCrawlingJobResponse, http_res)
        if utils.match_response(http_res, ["401", "403", "404", "4XX"], "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise errors.PipeshubDefaultError(
                "API error occurred", http_res, http_res_text
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise errors.PipeshubDefaultError(
                "API error occurred", http_res, http_res_text
            )

        raise errors.PipeshubDefaultError("Unexpected response received", http_res)

    def pause(
        self,
        *,
        connector: str,
        connector_id: str,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.PauseCrawlingJobResponse:
        r"""Pause a crawling job

        Pause a running or scheduled crawling job without losing its configuration.<br><br>

        <b>Overview:</b><br>
        Pausing a job stores its complete configuration and removes it from the active queue.
        The job can be resumed later with <code>POST /crawlingManager/{connector}/{connectorId}/resume</code>,
        which will restore the exact same schedule configuration.<br><br>

        <b>How Pausing Works:</b><br>
        <ol>
        <li>Current job configuration is stored in memory</li>
        <li>Active/repeatable job is removed from BullMQ queue</li>
        <li>Job state changes to \"paused\"</li>
        <li>No new job executions will occur until resumed</li>
        </ol>

        <b>Use Cases:</b><br>
        <ul>
        <li>Temporarily stop crawling during maintenance</li>
        <li>Pause data sync while investigating issues</li>
        <li>Stop crawling for a connector being reconfigured</li>
        </ul>

        <b>Note:</b> If a job is currently active (processing), it will complete before pausing.


        :param connector: Connector type identifier
        :param connector_id: Unique identifier of the connector instance
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.PauseCrawlingJobRequest(
            connector=connector,
            connector_id=connector_id,
        )

        req = self._build_request(
            method="POST",
            path="/crawlingManager/{connector}/{connectorId}/pause",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            allow_empty_value=None,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="pauseCrawlingJob",
                oauth2_scopes=None,
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["400", "401", "403", "404", "4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return unmarshal_json_response(models.PauseCrawlingJobResponse, http_res)
        if utils.match_response(http_res, ["400", "401", "403", "404", "4XX"], "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise errors.PipeshubDefaultError(
                "API error occurred", http_res, http_res_text
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise errors.PipeshubDefaultError(
                "API error occurred", http_res, http_res_text
            )

        raise errors.PipeshubDefaultError("Unexpected response received", http_res)

    async def pause_async(
        self,
        *,
        connector: str,
        connector_id: str,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.PauseCrawlingJobResponse:
        r"""Pause a crawling job

        Pause a running or scheduled crawling job without losing its configuration.<br><br>

        <b>Overview:</b><br>
        Pausing a job stores its complete configuration and removes it from the active queue.
        The job can be resumed later with <code>POST /crawlingManager/{connector}/{connectorId}/resume</code>,
        which will restore the exact same schedule configuration.<br><br>

        <b>How Pausing Works:</b><br>
        <ol>
        <li>Current job configuration is stored in memory</li>
        <li>Active/repeatable job is removed from BullMQ queue</li>
        <li>Job state changes to \"paused\"</li>
        <li>No new job executions will occur until resumed</li>
        </ol>

        <b>Use Cases:</b><br>
        <ul>
        <li>Temporarily stop crawling during maintenance</li>
        <li>Pause data sync while investigating issues</li>
        <li>Stop crawling for a connector being reconfigured</li>
        </ul>

        <b>Note:</b> If a job is currently active (processing), it will complete before pausing.


        :param connector: Connector type identifier
        :param connector_id: Unique identifier of the connector instance
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.PauseCrawlingJobRequest(
            connector=connector,
            connector_id=connector_id,
        )

        req = self._build_request_async(
            method="POST",
            path="/crawlingManager/{connector}/{connectorId}/pause",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            allow_empty_value=None,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="pauseCrawlingJob",
                oauth2_scopes=None,
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["400", "401", "403", "404", "4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return unmarshal_json_response(models.PauseCrawlingJobResponse, http_res)
        if utils.match_response(http_res, ["400", "401", "403", "404", "4XX"], "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise errors.PipeshubDefaultError(
                "API error occurred", http_res, http_res_text
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise errors.PipeshubDefaultError(
                "API error occurred", http_res, http_res_text
            )

        raise errors.PipeshubDefaultError("Unexpected response received", http_res)

    def resume(
        self,
        *,
        connector: str,
        connector_id: str,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.ResumeCrawlingJobResponse:
        r"""Resume a paused crawling job

        Resume a previously paused crawling job using its stored configuration.<br><br>

        <b>Overview:</b><br>
        Restores a paused job to active state using the exact configuration it had when paused.
        A new job is created in BullMQ with the same schedule settings.<br><br>

        <b>How Resuming Works:</b><br>
        <ol>
        <li>Retrieve stored job configuration from pause state</li>
        <li>Create new scheduled job with same configuration</li>
        <li>Remove from paused jobs tracking</li>
        <li>Job will execute according to its original schedule</li>
        </ol>

        <b>Note:</b> The job will resume according to its schedule, not immediately execute
        (unless it's a one-time job that hasn't run yet).


        :param connector: Connector type identifier
        :param connector_id: Unique identifier of the connector instance
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.ResumeCrawlingJobRequest(
            connector=connector,
            connector_id=connector_id,
        )

        req = self._build_request(
            method="POST",
            path="/crawlingManager/{connector}/{connectorId}/resume",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            allow_empty_value=None,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="resumeCrawlingJob",
                oauth2_scopes=None,
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["400", "401", "403", "404", "4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return unmarshal_json_response(models.ResumeCrawlingJobResponse, http_res)
        if utils.match_response(http_res, ["400", "401", "403", "404", "4XX"], "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise errors.PipeshubDefaultError(
                "API error occurred", http_res, http_res_text
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise errors.PipeshubDefaultError(
                "API error occurred", http_res, http_res_text
            )

        raise errors.PipeshubDefaultError("Unexpected response received", http_res)

    async def resume_async(
        self,
        *,
        connector: str,
        connector_id: str,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.ResumeCrawlingJobResponse:
        r"""Resume a paused crawling job

        Resume a previously paused crawling job using its stored configuration.<br><br>

        <b>Overview:</b><br>
        Restores a paused job to active state using the exact configuration it had when paused.
        A new job is created in BullMQ with the same schedule settings.<br><br>

        <b>How Resuming Works:</b><br>
        <ol>
        <li>Retrieve stored job configuration from pause state</li>
        <li>Create new scheduled job with same configuration</li>
        <li>Remove from paused jobs tracking</li>
        <li>Job will execute according to its original schedule</li>
        </ol>

        <b>Note:</b> The job will resume according to its schedule, not immediately execute
        (unless it's a one-time job that hasn't run yet).


        :param connector: Connector type identifier
        :param connector_id: Unique identifier of the connector instance
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.ResumeCrawlingJobRequest(
            connector=connector,
            connector_id=connector_id,
        )

        req = self._build_request_async(
            method="POST",
            path="/crawlingManager/{connector}/{connectorId}/resume",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            allow_empty_value=None,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="resumeCrawlingJob",
                oauth2_scopes=None,
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["400", "401", "403", "404", "4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return unmarshal_json_response(models.ResumeCrawlingJobResponse, http_res)
        if utils.match_response(http_res, ["400", "401", "403", "404", "4XX"], "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise errors.PipeshubDefaultError(
                "API error occurred", http_res, http_res_text
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise errors.PipeshubDefaultError(
                "API error occurred", http_res, http_res_text
            )

        raise errors.PipeshubDefaultError("Unexpected response received", http_res)

    def list_all_statuses(
        self,
        *,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.GetAllCrawlingJobStatusResponse:
        r"""Get all crawling job statuses

        Retrieve the status of all scheduled crawling jobs for the current organization.<br><br>

        <b>Overview:</b><br>
        Returns a list of all crawling jobs across all connectors for the authenticated user's
        organization. This includes active, waiting, paused, completed, and failed jobs.<br><br>

        <b>Response Details:</b><br>
        <ul>
        <li>Jobs are grouped by connector type</li>
        <li>Last 10 jobs per connector type are returned</li>
        <li>Includes both active queue jobs and paused jobs</li>
        </ul>

        <b>Use Cases:</b><br>
        <ul>
        <li>Dashboard overview of all crawling activities</li>
        <li>Monitoring job health across connectors</li>
        <li>Identifying failed or stuck jobs</li>
        </ul>


        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)
        req = self._build_request(
            method="GET",
            path="/crawlingManager/schedule/all",
            base_url=base_url,
            url_variables=url_variables,
            request=None,
            request_body_required=False,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            allow_empty_value=None,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="getAllCrawlingJobStatus",
                oauth2_scopes=None,
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["401", "4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return unmarshal_json_response(
                models.GetAllCrawlingJobStatusResponse, http_res
            )
        if utils.match_response(http_res, ["401", "4XX"], "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise errors.PipeshubDefaultError(
                "API error occurred", http_res, http_res_text
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise errors.PipeshubDefaultError(
                "API error occurred", http_res, http_res_text
            )

        raise errors.PipeshubDefaultError("Unexpected response received", http_res)

    async def list_all_statuses_async(
        self,
        *,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.GetAllCrawlingJobStatusResponse:
        r"""Get all crawling job statuses

        Retrieve the status of all scheduled crawling jobs for the current organization.<br><br>

        <b>Overview:</b><br>
        Returns a list of all crawling jobs across all connectors for the authenticated user's
        organization. This includes active, waiting, paused, completed, and failed jobs.<br><br>

        <b>Response Details:</b><br>
        <ul>
        <li>Jobs are grouped by connector type</li>
        <li>Last 10 jobs per connector type are returned</li>
        <li>Includes both active queue jobs and paused jobs</li>
        </ul>

        <b>Use Cases:</b><br>
        <ul>
        <li>Dashboard overview of all crawling activities</li>
        <li>Monitoring job health across connectors</li>
        <li>Identifying failed or stuck jobs</li>
        </ul>


        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)
        req = self._build_request_async(
            method="GET",
            path="/crawlingManager/schedule/all",
            base_url=base_url,
            url_variables=url_variables,
            request=None,
            request_body_required=False,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            allow_empty_value=None,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="getAllCrawlingJobStatus",
                oauth2_scopes=None,
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["401", "4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return unmarshal_json_response(
                models.GetAllCrawlingJobStatusResponse, http_res
            )
        if utils.match_response(http_res, ["401", "4XX"], "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise errors.PipeshubDefaultError(
                "API error occurred", http_res, http_res_text
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise errors.PipeshubDefaultError(
                "API error occurred", http_res, http_res_text
            )

        raise errors.PipeshubDefaultError("Unexpected response received", http_res)

    def remove_all(
        self,
        *,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.RemoveAllCrawlingJobsResponse:
        r"""Remove all crawling jobs

        Remove all scheduled crawling jobs for the current organization.<br><br>

        <b>Overview:</b><br>
        Bulk operation to remove all crawling jobs across all connectors for the organization.
        This is useful when decommissioning an organization or doing a complete reset.<br><br>

        <b>What Gets Removed:</b><br>
        <ul>
        <li>All active and waiting jobs</li>
        <li>All repeatable job configurations</li>
        <li>All paused jobs</li>
        <li>All job mappings for the organization</li>
        </ul>

        <b>Warning:</b> This operation cannot be undone. All job configurations will need
        to be recreated manually.


        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)
        req = self._build_request(
            method="DELETE",
            path="/crawlingManager/schedule/all",
            base_url=base_url,
            url_variables=url_variables,
            request=None,
            request_body_required=False,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            allow_empty_value=None,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="removeAllCrawlingJobs",
                oauth2_scopes=None,
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["401", "4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return unmarshal_json_response(
                models.RemoveAllCrawlingJobsResponse, http_res
            )
        if utils.match_response(http_res, ["401", "4XX"], "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise errors.PipeshubDefaultError(
                "API error occurred", http_res, http_res_text
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise errors.PipeshubDefaultError(
                "API error occurred", http_res, http_res_text
            )

        raise errors.PipeshubDefaultError("Unexpected response received", http_res)

    async def remove_all_async(
        self,
        *,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.RemoveAllCrawlingJobsResponse:
        r"""Remove all crawling jobs

        Remove all scheduled crawling jobs for the current organization.<br><br>

        <b>Overview:</b><br>
        Bulk operation to remove all crawling jobs across all connectors for the organization.
        This is useful when decommissioning an organization or doing a complete reset.<br><br>

        <b>What Gets Removed:</b><br>
        <ul>
        <li>All active and waiting jobs</li>
        <li>All repeatable job configurations</li>
        <li>All paused jobs</li>
        <li>All job mappings for the organization</li>
        </ul>

        <b>Warning:</b> This operation cannot be undone. All job configurations will need
        to be recreated manually.


        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)
        req = self._build_request_async(
            method="DELETE",
            path="/crawlingManager/schedule/all",
            base_url=base_url,
            url_variables=url_variables,
            request=None,
            request_body_required=False,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            allow_empty_value=None,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="removeAllCrawlingJobs",
                oauth2_scopes=None,
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["401", "4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return unmarshal_json_response(
                models.RemoveAllCrawlingJobsResponse, http_res
            )
        if utils.match_response(http_res, ["401", "4XX"], "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise errors.PipeshubDefaultError(
                "API error occurred", http_res, http_res_text
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise errors.PipeshubDefaultError(
                "API error occurred", http_res, http_res_text
            )

        raise errors.PipeshubDefaultError("Unexpected response received", http_res)
